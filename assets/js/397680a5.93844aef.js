"use strict";(globalThis.webpackChunkbook_frontend=globalThis.webpackChunkbook_frontend||[]).push([[762],{8241(e,n,i){i.r(n),i.d(n,{assets:()=>c,contentTitle:()=>a,default:()=>p,frontMatter:()=>s,metadata:()=>t,toc:()=>l});const t=JSON.parse('{"id":"vla-integration/voice-to-action-pipelines","title":"Chapter 1: Voice-to-Action Pipelines","description":"Learning Objectives","source":"@site/docs/vla-integration/01-voice-to-action-pipelines.md","sourceDirName":"vla-integration","slug":"/vla-integration/voice-to-action-pipelines","permalink":"/first-phase-hackathon/docs/vla-integration/voice-to-action-pipelines","draft":false,"unlisted":false,"editUrl":"https://github.com/your-username/your-project-name/tree/main/packages/create-docusaurus/templates/shared/docs/vla-integration/01-voice-to-action-pipelines.md","tags":[],"version":"current","sidebarPosition":1,"frontMatter":{"sidebar_position":1},"sidebar":"tutorialSidebar","previous":{"title":"Chapter 4: Perception-to-Navigation Flow","permalink":"/first-phase-hackathon/docs/isaac-ai-brain/perception-to-navigation"},"next":{"title":"Chapter 2: LLM-Based Cognitive Planning","permalink":"/first-phase-hackathon/docs/vla-integration/llm-cognitive-planning"}}');var o=i(4848),r=i(8453);const s={sidebar_position:1},a="Chapter 1: Voice-to-Action Pipelines",c={},l=[{value:"Learning Objectives",id:"learning-objectives",level:2},{value:"Introduction to Voice-to-Action Pipelines",id:"introduction-to-voice-to-action-pipelines",level:2},{value:"Key Components:",id:"key-components",level:3},{value:"OpenAI Whisper Integration",id:"openai-whisper-integration",level:2},{value:"Whisper Features:",id:"whisper-features",level:3},{value:"Integration Example:",id:"integration-example",level:3},{value:"Converting Voice Commands to Structured Intents",id:"converting-voice-commands-to-structured-intents",level:2},{value:"Intent Structure:",id:"intent-structure",level:3},{value:"Common Intents:",id:"common-intents",level:3},{value:"Summary",id:"summary",level:2},{value:"Exercises",id:"exercises",level:2}];function d(e){const n={code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,r.R)(),...e.components};return(0,o.jsxs)(o.Fragment,{children:[(0,o.jsx)(n.header,{children:(0,o.jsx)(n.h1,{id:"chapter-1-voice-to-action-pipelines",children:"Chapter 1: Voice-to-Action Pipelines"})}),"\n",(0,o.jsx)(n.h2,{id:"learning-objectives",children:"Learning Objectives"}),"\n",(0,o.jsx)(n.p,{children:"After completing this chapter, you will be able to:"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsx)(n.li,{children:"Understand voice-to-action pipeline concepts in robotics"}),"\n",(0,o.jsx)(n.li,{children:"Explain OpenAI Whisper integration for speech input"}),"\n",(0,o.jsx)(n.li,{children:"Describe converting voice commands to structured intents"}),"\n"]}),"\n",(0,o.jsx)(n.h2,{id:"introduction-to-voice-to-action-pipelines",children:"Introduction to Voice-to-Action Pipelines"}),"\n",(0,o.jsx)(n.p,{children:"Voice-to-action pipelines enable robots to understand spoken commands and convert them into executable actions. This technology bridges human language and robotic systems. This builds on the perception concepts introduced in Module 2 (Digital Twin) and the AI integration principles from Module 3 (Isaac AI Brain)."}),"\n",(0,o.jsx)(n.h3,{id:"key-components",children:"Key Components:"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Speech Recognition"}),": Converting audio to text"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Intent Classification"}),": Understanding command purpose"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Action Mapping"}),": Translating intent to robot actions"]}),"\n"]}),"\n",(0,o.jsx)(n.h2,{id:"openai-whisper-integration",children:"OpenAI Whisper Integration"}),"\n",(0,o.jsx)(n.p,{children:"OpenAI Whisper provides robust speech recognition capabilities for robotic applications."}),"\n",(0,o.jsx)(n.h3,{id:"whisper-features",children:"Whisper Features:"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsx)(n.li,{children:"Multilingual support"}),"\n",(0,o.jsx)(n.li,{children:"Noise robustness"}),"\n",(0,o.jsx)(n.li,{children:"Real-time processing capability"}),"\n",(0,o.jsx)(n.li,{children:"Context-aware transcription"}),"\n"]}),"\n",(0,o.jsx)(n.h3,{id:"integration-example",children:"Integration Example:"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:'import whisper\n\nmodel = whisper.load_model("base")\nresult = model.transcribe(audio_file)\ncommand_text = result["text"]\n'})}),"\n",(0,o.jsx)(n.h2,{id:"converting-voice-commands-to-structured-intents",children:"Converting Voice Commands to Structured Intents"}),"\n",(0,o.jsx)(n.p,{children:"Structured intents transform natural language into machine-readable commands."}),"\n",(0,o.jsx)(n.h3,{id:"intent-structure",children:"Intent Structure:"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{children:'{\n  "intent": "navigation",\n  "entities": {\n    "destination": "kitchen",\n    "confidence": 0.92\n  }\n}\n'})}),"\n",(0,o.jsx)(n.h3,{id:"common-intents",children:"Common Intents:"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsx)(n.li,{children:'Navigation commands ("Go to the kitchen")'}),"\n",(0,o.jsx)(n.li,{children:'Object manipulation ("Pick up the red cup")'}),"\n",(0,o.jsx)(n.li,{children:'Information requests ("What\'s on the table?")'}),"\n"]}),"\n",(0,o.jsx)(n.h2,{id:"summary",children:"Summary"}),"\n",(0,o.jsx)(n.p,{children:"Voice-to-action pipelines form the foundation of natural human-robot interaction, enabling intuitive control through spoken language."}),"\n",(0,o.jsx)(n.h2,{id:"exercises",children:"Exercises"}),"\n",(0,o.jsxs)(n.ol,{children:["\n",(0,o.jsx)(n.li,{children:"Identify three challenges in voice command interpretation."}),"\n",(0,o.jsx)(n.li,{children:"Design a simple intent structure for a navigation command."}),"\n"]})]})}function p(e={}){const{wrapper:n}={...(0,r.R)(),...e.components};return n?(0,o.jsx)(n,{...e,children:(0,o.jsx)(d,{...e})}):d(e)}},8453(e,n,i){i.d(n,{R:()=>s,x:()=>a});var t=i(6540);const o={},r=t.createContext(o);function s(e){const n=t.useContext(r);return t.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function a(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(o):e.components||o:s(e.components),t.createElement(r.Provider,{value:n},e.children)}}}]);