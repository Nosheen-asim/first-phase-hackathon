"use strict";(globalThis.webpackChunkbook_frontend=globalThis.webpackChunkbook_frontend||[]).push([[979],{952(n,e,i){i.r(e),i.d(e,{assets:()=>l,contentTitle:()=>r,default:()=>p,frontMatter:()=>o,metadata:()=>t,toc:()=>c});const t=JSON.parse('{"id":"vla-integration/vla-execution-ros2","title":"Chapter 3: VLA Execution with ROS 2","description":"Learning Objectives","source":"@site/docs/vla-integration/03-vla-execution-ros2.md","sourceDirName":"vla-integration","slug":"/vla-integration/vla-execution-ros2","permalink":"/first-phase-hackathon/docs/vla-integration/vla-execution-ros2","draft":false,"unlisted":false,"editUrl":"https://github.com/your-username/your-project-name/tree/main/packages/create-docusaurus/templates/shared/docs/vla-integration/03-vla-execution-ros2.md","tags":[],"version":"current","sidebarPosition":3,"frontMatter":{"sidebar_position":3},"sidebar":"tutorialSidebar","previous":{"title":"Chapter 2: LLM-Based Cognitive Planning","permalink":"/first-phase-hackathon/docs/vla-integration/llm-cognitive-planning"}}');var s=i(4848),a=i(8453);const o={sidebar_position:3},r="Chapter 3: VLA Execution with ROS 2",l={},c=[{value:"Learning Objectives",id:"learning-objectives",level:2},{value:"Introduction to VLA Execution with ROS 2",id:"introduction-to-vla-execution-with-ros-2",level:2},{value:"VLA Execution Components:",id:"vla-execution-components",level:3},{value:"Mapping Plans to ROS 2 Actions and Services",id:"mapping-plans-to-ros-2-actions-and-services",level:2},{value:"Action Mapping Process:",id:"action-mapping-process",level:3},{value:"ROS 2 Action Example:",id:"ros-2-action-example",level:3},{value:"Coordinating Perception, Navigation, and Manipulation",id:"coordinating-perception-navigation-and-manipulation",level:2},{value:"Coordination Elements:",id:"coordination-elements",level:3},{value:"Integration Patterns:",id:"integration-patterns",level:3},{value:"VLA Execution Patterns with ROS 2",id:"vla-execution-patterns-with-ros-2",level:2},{value:"Execution Patterns:",id:"execution-patterns",level:3},{value:"Example Integration:",id:"example-integration",level:3},{value:"Integration Examples and Best Practices",id:"integration-examples-and-best-practices",level:2},{value:"Best Practices:",id:"best-practices",level:3},{value:"Summary",id:"summary",level:2},{value:"Exercises",id:"exercises",level:2}];function d(n){const e={code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,a.R)(),...n.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(e.header,{children:(0,s.jsx)(e.h1,{id:"chapter-3-vla-execution-with-ros-2",children:"Chapter 3: VLA Execution with ROS 2"})}),"\n",(0,s.jsx)(e.h2,{id:"learning-objectives",children:"Learning Objectives"}),"\n",(0,s.jsx)(e.p,{children:"After completing this chapter, you will be able to:"}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsx)(e.li,{children:"Understand VLA execution with ROS 2 integration"}),"\n",(0,s.jsx)(e.li,{children:"Explain mapping plans to ROS 2 actions and services"}),"\n",(0,s.jsx)(e.li,{children:"Describe coordination of perception, navigation, and manipulation"}),"\n"]}),"\n",(0,s.jsx)(e.h2,{id:"introduction-to-vla-execution-with-ros-2",children:"Introduction to VLA Execution with ROS 2"}),"\n",(0,s.jsx)(e.p,{children:"Vision-Language-Action (VLA) execution integrates perception, language understanding, and action execution in ROS 2-based robotic systems. This combines the perception systems from Module 2 (Digital Twin), AI integration from Module 3 (Isaac AI Brain), and builds on the ROS 2 foundations from Module 1."}),"\n",(0,s.jsx)(e.h3,{id:"vla-execution-components",children:"VLA Execution Components:"}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Perception Pipeline"}),": Processing visual input"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Language Processing"}),": Understanding commands"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Action Execution"}),": Performing robotic tasks"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Coordination Layer"}),": Managing system integration"]}),"\n"]}),"\n",(0,s.jsx)(e.h2,{id:"mapping-plans-to-ros-2-actions-and-services",children:"Mapping Plans to ROS 2 Actions and Services"}),"\n",(0,s.jsx)(e.p,{children:"VLA systems translate high-level plans into ROS 2 actions and services."}),"\n",(0,s.jsx)(e.h3,{id:"action-mapping-process",children:"Action Mapping Process:"}),"\n",(0,s.jsxs)(e.ol,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Plan Interpretation"}),": Understanding action requirements"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Service Discovery"}),": Finding available ROS 2 services"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Action Binding"}),": Connecting plan elements to ROS 2 interfaces"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Execution Coordination"}),": Managing action sequencing"]}),"\n"]}),"\n",(0,s.jsx)(e.h3,{id:"ros-2-action-example",children:"ROS 2 Action Example:"}),"\n",(0,s.jsx)(e.pre,{children:(0,s.jsx)(e.code,{className:"language-python",children:"import rclpy\nfrom rclpy.action import ActionClient\nfrom nav2_msgs.action import NavigateToPose\n\nclass VLAAgent:\n    def __init__(self):\n        self.nav_client = ActionClient(self, NavigateToPose, 'navigate_to_pose')\n\n    def execute_navigation_action(self, goal_pose):\n        goal = NavigateToPose.Goal()\n        goal.pose = goal_pose\n        self.nav_client.send_goal_async(goal)\n"})}),"\n",(0,s.jsx)(e.h2,{id:"coordinating-perception-navigation-and-manipulation",children:"Coordinating Perception, Navigation, and Manipulation"}),"\n",(0,s.jsx)(e.p,{children:"VLA systems coordinate multiple subsystems for integrated behavior."}),"\n",(0,s.jsx)(e.h3,{id:"coordination-elements",children:"Coordination Elements:"}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Perception Services"}),": Object detection, scene understanding"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Navigation Services"}),": Path planning, motion execution"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Manipulation Services"}),": Grasping, manipulation actions"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"State Management"}),": Tracking system status"]}),"\n"]}),"\n",(0,s.jsx)(e.h3,{id:"integration-patterns",children:"Integration Patterns:"}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Sequential Execution"}),": Actions in predetermined order"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Conditional Execution"}),": Actions based on perception results"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Parallel Execution"}),": Concurrent perception and action"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Feedback Integration"}),": Adjusting based on execution results"]}),"\n"]}),"\n",(0,s.jsx)(e.h2,{id:"vla-execution-patterns-with-ros-2",children:"VLA Execution Patterns with ROS 2"}),"\n",(0,s.jsx)(e.p,{children:"Different execution patterns suit various VLA applications."}),"\n",(0,s.jsx)(e.h3,{id:"execution-patterns",children:"Execution Patterns:"}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Reactive"}),": Immediate response to language commands"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Deliberative"}),": Planning-intensive with multiple steps"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Hybrid"}),": Combining reactive and deliberative approaches"]}),"\n"]}),"\n",(0,s.jsx)(e.h3,{id:"example-integration",children:"Example Integration:"}),"\n",(0,s.jsx)(e.pre,{children:(0,s.jsx)(e.code,{children:'Language Command: "Bring me the red cup"\n1. Perception: Detect red cup location\n2. Navigation: Plan path to cup\n3. Manipulation: Grasp the cup\n4. Navigation: Plan path to user\n5. Manipulation: Deliver the cup\n'})}),"\n",(0,s.jsx)(e.h2,{id:"integration-examples-and-best-practices",children:"Integration Examples and Best Practices"}),"\n",(0,s.jsx)(e.p,{children:"Successful VLA integration requires careful design."}),"\n",(0,s.jsx)(e.h3,{id:"best-practices",children:"Best Practices:"}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Modular Design"}),": Separate perception, language, and action"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Error Handling"}),": Graceful degradation when components fail"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"State Consistency"}),": Maintain consistent system state"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Safety Considerations"}),": Prevent unsafe actions"]}),"\n"]}),"\n",(0,s.jsx)(e.h2,{id:"summary",children:"Summary"}),"\n",(0,s.jsx)(e.p,{children:"VLA execution with ROS 2 integrates perception, language understanding, and action execution into coherent robotic behaviors that respond to natural language commands."}),"\n",(0,s.jsx)(e.h2,{id:"exercises",children:"Exercises"}),"\n",(0,s.jsxs)(e.ol,{children:["\n",(0,s.jsx)(e.li,{children:"Design a VLA execution flow for a simple pick-and-place task."}),"\n",(0,s.jsx)(e.li,{children:"Explain how feedback from perception affects action execution."}),"\n"]})]})}function p(n={}){const{wrapper:e}={...(0,a.R)(),...n.components};return e?(0,s.jsx)(e,{...n,children:(0,s.jsx)(d,{...n})}):d(n)}},8453(n,e,i){i.d(e,{R:()=>o,x:()=>r});var t=i(6540);const s={},a=t.createContext(s);function o(n){const e=t.useContext(a);return t.useMemo(function(){return"function"==typeof n?n(e):{...e,...n}},[e,n])}function r(n){let e;return e=n.disableParentContext?"function"==typeof n.components?n.components(s):n.components||s:o(n.components),t.createElement(a.Provider,{value:e},n.children)}}}]);